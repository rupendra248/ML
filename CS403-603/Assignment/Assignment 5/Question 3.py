# -*- coding: utf-8 -*-
"""5_ML_3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/14Xhdl2hFYuwzvgV5oxooqhw3u1aceCRX
"""

from google.colab import drive
drive.mount('/content/drive')

import pandas as pd

from numpy import loadtxt
from keras.models import Sequential
from keras.layers import Dense

#Importing the dataset
Train_X = pd.read_csv('/content/drive/MyDrive/CSV/TrainX.csv', delimiter=',')
Train_y = pd.read_csv('/content/drive/MyDrive/CSV/TrainY.csv', delimiter=',')
Test_X = pd.read_csv('/content/drive/MyDrive/CSV/TestX.csv', delimiter=',')
Test_y = pd.read_csv('/content/drive/MyDrive/CSV/TestY.csv', delimiter=',')

#required libraries
import numpy as np
import math
import matplotlib.pyplot as plt
import matplotlib.colors
import time
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, mean_squared_error, log_loss
from tqdm import tqdm_notebook 

from IPython.display import HTML
import warnings
from sklearn.preprocessing import OneHotEncoder
from sklearn.datasets import make_blobs

import torch
import torch.nn.functional as F
warnings.filterwarnings('ignore')

#converting the numpy array to torch tensors
#Train_y --> (5,6) -- (0,1)
#Test_y --> (5,6) -- (0,1)
for x in range(len(Train_y)):
  if Train_y[x,] == 5:
    Train_y[x,] = 0.0
  else:
    Train_y[x,] = 1.0

for x in range(len(Test_y)):
  if Test_y[x,] == 5:
    Test_y[x,] = 0.0
  else:
    Test_y[x,] = 1.0


X_train = torch.tensor(Train_X).float()
Y_train = torch.tensor(Train_y).long()
X_val = torch.tensor(Test_X).float()
Y_val = torch.tensor(Test_y).long()
print(X_train.shape, Y_train.shape)

#function for computing forward pass in the network
def model(x):
    A1 = torch.matmul(x, weights1) + bias1 # 753 * 5 
    H1 = A1.sigmoid() # 753 * 5 
    A2 = torch.matmul(H1, weights2) + bias2 # (N, 2) x (2, 4) -> (N, 4)
    H2 = A2.exp()/A2.exp().sum(-1).unsqueeze(-1) # (N, 4) #applying softmax at output layer.
    return H2

#function to calculate loss of a function.
#y_hat -> predicted & y -> actual
def loss_fn(y_hat, y):
     return -(y_hat[range(y.shape[0]), y].log()).mean()

#function to calculate accuracy of model
def accuracy(y_hat, y):
     pred = torch.argmax(y_hat, dim=1)
     return (pred == y).float().mean()

torch.manual_seed(0)
weights1 = (torch.randn(64, 5) / math.sqrt(2)).float()
weights1.requires_grad_()
bias1 = torch.zeros(5, requires_grad=True)
weights2 = torch.randn(5, 2) / math.sqrt(2)
weights2.requires_grad_()
bias2 = torch.zeros(2, requires_grad=True)

learning_rate = 0.2
epochs = 30
loss_arr = []
acc_arr = []

for epoch in range(epochs):
    y_hat = model(X_train)
    loss = F.cross_entropy(y_hat, Y_train)
    loss.backward()
    loss_arr.append(loss.item())
    acc_arr.append(accuracy(y_hat, Y_train))

    with torch.no_grad():
        weights1 -= weights1.grad * learning_rate
        bias1 -= bias1.grad * learning_rate
        weights2 -= weights2.grad * learning_rate
        bias2 -= bias2.grad * learning_rate
        weights1.grad.zero_()
        bias1.grad.zero_()
        weights2.grad.zero_()
        bias2.grad.zero_()

loss_arr, acc_arr